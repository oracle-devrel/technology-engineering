{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439e0c6d-49d1-4ea0-a418-5235e3a7e22f",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5168763-0ba6-4a22-95f4-b40e2b7c63e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(\"resource_principal\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cbdef-5ee8-4c31-be30-694143407a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Spark Snowflake to OCI\") \\\n",
    "        .config('spark.jars.packages', 'net.snowflake:spark-snowflake_2.12:2.14.0-spark_3.2') \\\n",
    "        .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "namespace = 'fro8fl9kuqli'\n",
    "bucket = 'snowflake'\n",
    "folder = 'electrodata'\n",
    "\n",
    "output_path='oci://'+bucket+'@'+namespace+'/'+folder\n",
    "\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1865f0d2-5cbd-45cf-977f-d7b9dfb90efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#print(spark.sparkContext._jsc.sc().listJars())\n",
    "print(spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69fad8-16ad-4213-b380-b3f13c80880d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "st = (spark.read\n",
    "  .format(\"snowflake\")\n",
    "  .option(\"dbtable\", \"tabLE\")\n",
    "  .option(\"sfUrl\", \"XXXXXXX.snowflakecomputing.com\")\n",
    "  .option(\"sfUser\", \"TEST\")\n",
    "  .option(\"sfPassword\", \"PASS\")\n",
    "  .option(\"sfDatabase\", \"TEST\")\n",
    "  .option(\"sfSchema\", \"TEST\")\n",
    "  .option(\"sfWarehouse\", \"COMPUTE_WH\")\n",
    "  .load()\n",
    ").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a795d6ea-f1bc-4440-9599-d28903a51dce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695\n"
     ]
    }
   ],
   "source": [
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98bbd83-f248-4b87-b79c-12c624b81fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oci://streamdata@fro8fl9kuqli/ele_sample.json\n",
      "root\n",
      " |-- phase: string (nullable = true)\n",
      " |-- times: string (nullable = true)\n",
      " |-- topik: string (nullable = true)\n",
      " |-- v: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##inputpath='oci://'+bucket+'@'+namespace+'/ele_sample.json'\n",
    "\n",
    "inoutpath = f'oci://streamdata@fro8fl9kuqli/ele_sample.json'\n",
    "print(inputpath)\n",
    "\n",
    "example = spark.read.json(inputpath)\n",
    "schema = example.schema.json()\n",
    "example.printSchema()\n",
    "# print(schema)\n",
    "# example.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c87bf-764d-4804-b238-a38de78c0f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, from_json, col, concat, explode, year, month, date_format, to_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Reading raw Kafka stream.\n",
    "kafka = spark.readStream.format('kafka').options(**raw_kafka_options).load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"), col(\"timestamp\").cast(\"timestamp\").alias(\"kafka_time\"),col(\"offset\").cast(\"int\").alias(\"offset\"))\n",
    "\n",
    "#kafka.printSchema()\n",
    "\n",
    "df = kafka \\\n",
    "    .select(\"*\") \\\n",
    "    .select(\"data.*\",\"kafka_time\",\"offset\") \\\n",
    "    .select(\"kafka_time\",\"offset\",\"phase\",\"v\",\"times\",\"topik\") \\\n",
    "    .withColumn('year',year(\"times\")) \\\n",
    "    .withColumn('month',date_format('times', 'yyyy-MM')) \\\n",
    "    .withColumn('day',date_format('times', 'yyyy-MM-dd')) \\\n",
    "    .withColumn('hour',date_format('times', 'HH')) \\\n",
    "    .withColumn(\"times\",to_timestamp(\"times\"))\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='1 minute') \\\n",
    "    .foreachBatch(lambda df, epoch_id: df.write.mode(\"append\").partitionBy(\"year\",\"month\",\"day\").parquet(f\"{output_path}\")) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e2d6379-918f-473a-ac9c-68f0c54dd888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query.stop()\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c143fca7-7d64-4545-9845-11ae7d95799b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15897"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = spark.read.option(\"basePath\", output_path).parquet(output_path+\"/year=*/month=*/day=2024-05-23/*.parquet\")\n",
    "#dl.printSchema()\n",
    "dl.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac6bf0cf-845d-4e60-8404-f44a86ac56be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|hour|             sum(v)|\n",
      "+----+-------------------+\n",
      "|  00| 115964.67999999959|\n",
      "|  01|  123614.4700000002|\n",
      "|  02| 128611.23999999996|\n",
      "|  03| 160143.60000000024|\n",
      "|  04|  253690.7400000005|\n",
      "|  05| 153279.22999999995|\n",
      "|  06| -71796.88999999997|\n",
      "|  07|-11835.169999999998|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#dl.createOrReplaceTempView(\"eletro\")\n",
    "\n",
    "sql=\"select hour, sum(v) from eletro group by hour order by 1\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475557a-743d-484b-8caf-5b0df8e840dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_str=\"select day, action, count(*)\" \\\n",
    " \" from bal_tra \"  \\\n",
    " \" group by day, action\"\n",
    "\n",
    "# # Execute SQL\n",
    "dc = spark.sql(sql_str)\n",
    "dc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06f91e-18bb-4609-9665-1258d7508aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigbet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b9b66-31f7-4173-a606-a72d1d87445f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, sum,avg,max,count\n",
    "dl.groupBy(\"action\").agg(count(\"*\")).orderBy(\"action\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b737ec7-7e1f-4f79-ba89-b368279c13a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af43f036-9dce-46fe-9ce6-9c3c8da072e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+--------------+-------------+----------+----------+\n",
      "|WINDOW_START|WINDOW_END|LASTTIMESTAMP|FIRSTTIMESTAMP|DEPOSIT_COUNT|MAXDEPOSIT|MINDEPOSIT|\n",
      "+------------+----------+-------------+--------------+-------------+----------+----------+\n",
      "+------------+----------+-------------+--------------+-------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dl = spark.read.option(\"basePath\", output_path).parquet(output_path+\"/*.parquet\") \n",
    "# \\\n",
    "#         .filter(col(\"day\")=='2024-04-22').filter(col(\"hour\")==\"9\")\n",
    "\n",
    "dl.count()\n",
    "\n",
    "dl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1b231-a12d-4a7b-8586-50f749353003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl.createOrReplaceTempView(\"dl\")\n",
    "\n",
    "sql=\"select * from bal_tra\"\n",
    "#spark.sql(sql).show(2)\n",
    "\n",
    "\n",
    "sql_str=\"select day, count(*)\" \\\n",
    " \" from dl \"  \\\n",
    " \" group by day\"\n",
    "\n",
    "# # Execute SQL\n",
    "dc = spark.sql(sql_str)\n",
    "dc.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db3838-ae51-4cf6-a353-0dda1260e453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "from autovizwidget.widget.utils import display_dataframe\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "pdc = dc.toPandas()\n",
    "display_dataframe(pdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed41ea-4428-4b35-86f1-294fc6aa4f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "inter = (dl.filter(col(\"action\")=='deposit')\n",
    "    .withWatermark(\"Created_at\", \"1 minute\")\n",
    "    .groupBy(['action', F.window('Created_at', '1 minute')])\n",
    "    .agg(\n",
    "         F.expr(\"max_by(amount_cents, Created_at)\").alias('lastvalue'),\n",
    "         F.expr(\"min_by(amount_cents, Created_at)\").alias('firstvalue'),\n",
    "         F.max('Created_at').alias('lastTimeStamp'),\n",
    "         F.count('Created_at').alias('trans_qty'),\n",
    "         F.min('Created_at').alias('firstTimeStamp'),\n",
    "         F.max('amount_cents').alias('MaxBet'),\n",
    "         F.min('amount_cents').alias('MinBet'),\n",
    "    )\n",
    ").orderBy(\"lastTimeStamp\", ascending=False)\n",
    "\n",
    "\n",
    "inter.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee14bd-81cc-46ed-b782-393f1e43714a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fec87-1912-4be7-b252-a4d12540cf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a2859-f034-4101-80e7-1add51c40c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83ae92-410f-4d81-9861-6169ddabe2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "TOKEN = \"6671512971:AAEjIUEFxAcuK5pCl0EinBm8MDQ-s0csDl8\"\n",
    "# url = f\"https://api.telegram.org/bot{TOKEN}/getUpdates\"\n",
    "# print(requests.get(url).json())\n",
    "\n",
    "chat_id = \"844904100\"\n",
    "message = \"OCI Python can send a message to your telegram chat!\"\n",
    "url = f\"https://api.telegram.org/bot{TOKEN}/sendMessage?chat_id={chat_id}&text={message}\"\n",
    "print(requests.get(url).json()) # this sends the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2f8f9-83a0-4433-b7a1-a0438de05699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_telegram_message(token, chat_id, message):\n",
    "    url = f\"https://api.telegram.org/bot{token}/sendMessage\"\n",
    "    payload = {\n",
    "        'chat_id': chat_id,\n",
    "        'text': message\n",
    "        }\n",
    "    response = requests.post(url, data=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Example usage:\n",
    "bot_token = '6671512971:AAEjIUEFxAcuK5pCl0EinBm8MDQ-s0csDl8'\n",
    "chat_id = '844904100'\n",
    "message = 'Hello message nr 5!'\n",
    "\n",
    "a = send_telegram_message(bot_token, chat_id, message)\n",
    "#print(a['ok'])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325323e1-f8ba-46ae-8425-95345ad35bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.format(\"oracle\") \\\n",
    "    .option(\"walletUri\",\"oci://dataflow_app@\"+namespace+\"/Adw_Forza_wallet.zip\") \\\n",
    "    .option(\"connectionId\",\"db201909271450_high\") \\\n",
    "    .option(\"query\", \"select * from car.kafka_stream_dv\") \\\n",
    "    .option(\"user\", \"CAR\")\\\n",
    "    .option(\"password\", \"WelcomeBack123#\")\\\n",
    "    .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75583-1a21-446e-9951-4251c50a17ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v3]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
