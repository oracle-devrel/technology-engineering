{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "gpu_memory_utilization": 0.98,
    "tensor_parallel_size": 1,
    "max_model_len": 8192,
    "max_num_batched_tokens": 8192
}
